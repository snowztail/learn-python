{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will explore some techniques for making use of *unstructured* text data in supervised and unsupervised learning. The techniques introduced come from the fields of *information retrieval* (IR) and *natural language processing* (NLP).\n",
    "\n",
    "Each data point consists of a single text, called a *document*.\n",
    "\n",
    "The set of all documents in the analysis is called a *corpus*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The corpus\n",
    "\n",
    "We will look at a set of user-contributed movie reviews retrieved from IMDb (The Internet Movie Database). Each document is the text of one review, together with a label indicating whether the review is broadly \"positive\" or \"negative\".\n",
    "\n",
    "The data provided here are derived from the dataset available at http://ai.stanford.edu/~amaas/data/sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, you will need to unpack the archived dataset `imdb.zip`.\n",
    "\n",
    "The unpacked data contains a `train` and a `test` directory, each of which contains positive and negative examples.\n",
    "\n",
    "`scikit-learn` can directly load the labelled corpus from this directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/snowztail/Repositories/learn-python/machine-learning-with-python/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"../assets/imdb/train/\")\n",
    "# load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test = load_files(\"../assets/imdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3445861 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "# Transform text to sparse matrices\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 74849\n",
      "First 20 features:\n",
      "['00' '000' '0000000000001' '00001' '00015' '000s' '001' '003830' '006'\n",
      " '007' '0079' '0080' '0083' '0093638' '00am' '00pm' '00s' '01' '01pm' '02']\n",
      "Features 20010 to 20030:\n",
      "['dratted' 'draub' 'draught' 'draughts' 'draughtswoman' 'draw' 'drawback'\n",
      " 'drawbacks' 'drawer' 'drawers' 'drawing' 'drawings' 'drawl' 'drawled'\n",
      " 'drawling' 'drawn' 'draws' 'draza' 'dre' 'drea']\n",
      "Every 2000th feature:\n",
      "['00' 'aesir' 'aquarian' 'barking' 'blustering' 'bête' 'chicanery'\n",
      " 'condensing' 'cunning' 'detox' 'draper' 'enshrined' 'favorit' 'freezer'\n",
      " 'goldman' 'hasan' 'huitieme' 'intelligible' 'kantrowitz' 'lawful' 'maars'\n",
      " 'megalunged' 'mostey' 'norrland' 'padilla' 'pincher' 'promisingly'\n",
      " 'receptionist' 'rivals' 'schnaas' 'shunning' 'sparse' 'subset'\n",
      " 'temptations' 'treatises' 'unproven' 'walkman' 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(solver='lbfgs', max_iter=1000), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n",
      "Best parameters:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Exhaustive search over specified parameter values for an estimator\n",
    "grid = GridSearchCV(LogisticRegression(solver='newton-cg', max_iter=100), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.88\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "\n",
    "print(\"Test score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retry with `min_df` constraint: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <25000x27272 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3368680 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 features:\n",
      "['00' '000' '007' '00s' '01' '02' '03' '04' '05' '06' '07' '08' '09' '10'\n",
      " '100' '1000' '100th' '101' '102' '103' '104' '105' '107' '108' '10s'\n",
      " '10th' '11' '110' '112' '116' '117' '11th' '12' '120' '12th' '13' '135'\n",
      " '13th' '14' '140' '14th' '15' '150' '15th' '16' '160' '1600' '16mm' '16s'\n",
      " '16th']\n",
      "Features 20010 to 20030:\n",
      "['repent' 'repentance' 'repercussions' 'repertoire' 'repetition'\n",
      " 'repetitions' 'repetitious' 'repetitive' 'rephrase' 'replace' 'replaced'\n",
      " 'replacement' 'replaces' 'replacing' 'replay' 'replayable' 'replayed'\n",
      " 'replaying' 'replays' 'replete']\n",
      "Every 700th feature:\n",
      "['00' 'affections' 'appropriately' 'barbra' 'blurbs' 'butcher' 'cheery'\n",
      " 'commit' 'courtroom' 'deconstruct' 'disgraced' 'dvd' 'escapist' 'felix'\n",
      " 'freeze' 'gorier' 'haunts' 'hungarian' 'insincere' 'juggernaut' 'leer'\n",
      " 'mae' 'messes' 'mushy' 'occasion' 'parker' 'pleasantly' 'pronto' 'recipe'\n",
      " 'reviewing' 'saruman' 'she' 'sneering' 'stefano' 'swashbuckling' 'thrust'\n",
      " 'tvm' 'vampirism' 'westernized']\n"
     ]
    }
   ],
   "source": [
    "# Compared to first 20 features above, those appearing at least 5 times seem more informative\n",
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n",
      "Best parameters:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(solver='newton-cg', max_iter=100), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "type of elements of text_train: <class 'bytes'>\n",
      "length of text_train: 25000\n",
      "text_train[6]:\n",
      "b\"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\"\n"
     ]
    }
   ],
   "source": [
    "# text_train is a list, where individual elements are immutable \"bytes\"\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"type of elements of text_train: {}\".format(type(text_train[0])))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[6]:\\n{}\".format(text_train[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`text_train` is a `list` of length 25000.\n",
    "\n",
    "The individual documents are stored as type `bytes` - i.e. immutable bytestrings that are interpreted as Unicode text data. See [docs](https://docs.python.org/3/library/stdtypes.html#bytes-objects) and [explanation](https://stackoverflow.com/questions/6269765/what-does-the-b-character-do-in-front-of-a-string-literal).\n",
    "\n",
    "A `bytes` literal looks like a string literal with a `b` prepended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is balanced, with equal numbers of positive and negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples per class (training): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we should remove the `<br />` tags, which just represent line breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load and clean up the test data in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The file `reviews.json` contains expert reviews for papers submitted to an international conference on computing and informatics.\n",
    "\n",
    "*Appel, Orestes & Chiclana, Francisco & Carter, Jenny & Fujita, Hamido., 2016. A hybrid approach to sentiment analysis.*\n",
    "\n",
    "The data is held in JSON format, which is a flexible format for structured data. Here's how we can extract the reviews into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load data using Python JSON module\n",
    "with open('../assets/reviews.json','r') as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "reviews = pd.json_normalize(data, record_path=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['confidence', 'evaluation', 'id', 'lan', 'orientation', 'remarks',\n",
       "       'text', 'timespan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>id</th>\n",
       "      <th>lan</th>\n",
       "      <th>orientation</th>\n",
       "      <th>remarks</th>\n",
       "      <th>text</th>\n",
       "      <th>timespan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>- El artículo aborda un problema contingente y...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>El artículo presenta recomendaciones prácticas...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>- El tema es muy interesante y puede ser de mu...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>Se explica en forma ordenada y didáctica una e...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  confidence evaluation  id lan orientation remarks  \\\n",
       "0          4          1   1  es           0           \n",
       "1          4          1   2  es           1           \n",
       "2          5          1   3  es           1           \n",
       "3          4          2   1  es           1           \n",
       "4          4          2   2  es           0           \n",
       "\n",
       "                                                text    timespan  \n",
       "0  - El artículo aborda un problema contingente y...  2010-07-05  \n",
       "1  El artículo presenta recomendaciones prácticas...  2010-07-05  \n",
       "2  - El tema es muy interesante y puede ser de mu...  2010-07-05  \n",
       "3  Se explica en forma ordenada y didáctica una e...  2010-07-05  \n",
       "4                                                     2010-07-05  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `text` column contains the text of each review, whilst the `evaluation` column is a numerical score for each paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a training and testing dataset containing only the Spanish language (`lan == es`) reviews. Later, we will use these documents to attempt regression analysis to predict the `evaluation` score, which you should also extract as the target values. \n",
    "\n",
    "*Notes*\n",
    "\n",
    "It's fine to use strings for the documents rather than the `bytes` datatype we saw earlier. Remember that scikit-learn can handle pandas `Series` data without needing to unpack it.\n",
    "\n",
    "You will need the DataFrame method `query()`.\n",
    "\n",
    "Consider any basic cleaning operations you can sensibly apply to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lan\n",
       "0    es\n",
       "1    es\n",
       "2    es\n",
       "3    es\n",
       "4    es\n",
       "..   ..\n",
       "400  es\n",
       "401  es\n",
       "402  es\n",
       "403  es\n",
       "404  es\n",
       "\n",
       "[405 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[[\"lan\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_es = reviews.query('lan == \"es\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      - El artículo aborda un problema contingente y...\n",
       "1      El artículo presenta recomendaciones prácticas...\n",
       "2      - El tema es muy interesante y puede ser de mu...\n",
       "3      Se explica en forma ordenada y didáctica una e...\n",
       "4                                                       \n",
       "                             ...                        \n",
       "400    El trabajo pretende ofrecer una visión del uso...\n",
       "401    El paper está bien escrito y de fácil lectura....\n",
       "402    Observación de fondo:  No se presenta un ejemp...\n",
       "403    Se propone un procedimiento para elaborar máqu...\n",
       "404    El artículo describe básicamente los component...\n",
       "Name: text, Length: 388, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_es.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_test, y_train, y_test = train_test_split(reviews_es.text, reviews_es.evaluation, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizer = CountVectorizer(min_df=2, token_pattern=r'(?u)\\b\\w\\w+\\b').fit(text_train)\n",
    "vectorizer = CountVectorizer(min_df=1, token_pattern=r'(?u)\\b[^_\\d\\W][^_\\d\\W]+\\b').fit(text_train)\n",
    "X_train = vectorizer.transform(text_train)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aanalizar', 'abarca', 'abarcando', ..., 'único', 'útil', 'útiles'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores_train = cross_val_score(LogisticRegression(solver='lbfgs', max_iter=1000), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.36\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(text_test)\n",
    "\n",
    "scores_test = cross_val_score(LogisticRegression(solver='lbfgs', max_iter=1000), X_test, y_test, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores_test)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
